{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes has this interesting relation with PCA.\n",
    "# PCA makes sure that all the principal components be normal to each other, that means their correlation is zero.\n",
    "# If the principal components are gaussian then naive assumption of Naive BAyes become true.\n",
    "# although just to be clear, 0 correlation does not mean independence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "# from sklearn.naive_bayes import GaussianNB # doesn't have smoothing\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from future.utils import iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB(object):\n",
    "\n",
    "    def fit(self, X, Y, smoothing=1e-2):\n",
    "        self.gaussians = dict()\n",
    "        self.priors = dict()\n",
    "\n",
    "        # getting labels for y\n",
    "        labels = set(Y)\n",
    "        \n",
    "        for c in labels:\n",
    "            # for all the unique labels\n",
    "            # get all the x for which label == c\n",
    "            current_x = X[Y == c]\n",
    "            # calculate and assign mean and variance for each class, for each feature (each class has vector of 784)\n",
    "            # smoothing is added to avoid nan\n",
    "            self.gaussians[c] = {\n",
    "                'mean': current_x.mean(axis=0),\n",
    "                'var': current_x.var(axis=0) + smoothing,\n",
    "            }\n",
    "            # calculating priors, just probability of a class\n",
    "            self.priors[c] = float(len(Y[Y == c])) / len(Y)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        P = self.predict(X)\n",
    "        return np.mean(P == Y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        N, D = X.shape\n",
    "        \n",
    "        # length of class - number of classes\n",
    "        K = len(self.gaussians)\n",
    "        \n",
    "        # probability matrix - for n numbers and for each class\n",
    "        P = np.zeros((N, K))\n",
    "        \n",
    "        # for each gaussian or each class distribution, get the mean and variance \n",
    "        # explanation of below lines required....\n",
    "        # we calculate log posterior probability for each class\n",
    "        for c, g in iteritems(self.gaussians):\n",
    "            mean, var = g['mean'], g['var']\n",
    "            P[:,c] = mvn.logpdf(X, mean=mean, cov=var) + np.log(self.priors[c])\n",
    "        # to get the prediction, we take the argmax of P - P contains 'c' columns \n",
    "        return np.argmax(P, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../unsupervised_learning_clustering_part1/data/mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB train score: 0.6151190476190476\n",
      "NB test score: 0.6133888888888889\n"
     ]
    }
   ],
   "source": [
    "# try NB by itself\n",
    "model1 = GaussianNB()\n",
    "model1.fit(X, y)\n",
    "print(\"NB train score:\", model1.score(X_train, y_train))\n",
    "print(\"NB test score:\", model1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try NB with PCA first\n",
    "pca = PCA(n_components=50)\n",
    "Z_train = pca.fit_transform(X_train)\n",
    "Z_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB+PCA train score: 0.870452380952381\n",
      "NB+PCA test score: 0.8677777777777778\n"
     ]
    }
   ],
   "source": [
    "model2 = GaussianNB()\n",
    "model2.fit(Z_train, y_train)\n",
    "print(\"NB+PCA train score:\", model2.score(Z_train, y_train))\n",
    "print(\"NB+PCA test score:\", model2.score(Z_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see, scores have been increased, so naive bayes works better here after we do PCA preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full covariance vs diagonal covariance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
